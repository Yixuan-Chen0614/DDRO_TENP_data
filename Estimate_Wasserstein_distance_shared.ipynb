{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate Wasserstein distance discribing envolutionary changes\n",
    "Methods in [1] were adopted, whose source code can be found in [2]. Besides, data in 'df_n','df_g' were also downloaded from [2].<br/>\n",
    "This version was updated in Dec., 2020.\n",
    "\n",
    "### This code aims to extract Wasserstein distance discribing envolutionary changes from data in Australia National Electricity Market.###\n",
    "\n",
    "#### Recall data preparation for IEEE RTS 24-node model embeding Australia NEM data [3],[4]: ####\n",
    "\n",
    "In total, the NEM generator dataset contains technical and economic information relating to 203 generating units （conventional generators and renewable energy units, each has a dispatchable unit ID）, while the network dataset consists of 912 nodes, and 1406 AC edges with line voltages in the range of 110 kV to 500 kV.\n",
    "There are ***5*** trading regions (NSW,QLD,SA,TAS,VIC) and ***16*** NEM zones in the operation devided by AEMO. To facilitate our transmission network planning study, We take this 16-node model by assuming that the power balance constriants are met in each NEM zone.Therefore, the load and wind power signals are aggragated in these 16 nodes based on their geographic positions.\n",
    "\n",
    "In the original IEEE RTS 24-node model, it has 17 load nodes while no wind power. <br/>\n",
    "In the modified IEEE RTS 24-node model, technical and economic information relating to generators and topology of existing lines are not changed. However, to be compatible with the NEM dataset: <br/> (1) load demand at #13 node (reference node) is set to be zero (so that we get a 16-load model);  <br/> (2) #11,#12,#17,#24 are selected as nodes equipped with wind power units.  \n",
    "\n",
    "#### Note that, in estimating  Wasserstein distance discribing envolutionary changes:####\n",
    " \n",
    " (1) Alothough a 16-node load vector is considered, we estimate the Wasserstein distance using the load data aggregated at the five trading regions, since the detialed load data at each node is not avialiable;<br />\n",
    " (2) Although a 4-node wind power vector is considered, we estimate the Wasserstein distance using data at site \"NSA\", since wind power at the remaining sites are negligible in early years, say, the year of 2011.\n",
    "\n",
    "### Basic procedures ###\n",
    "1. Get the standard version of variable injections, $P_v=[P_D;P_W]$  denoted as $z$;\n",
    "2. Fit distributions of $z$ as Gaussian mixture models (GMMs);\n",
    "3. Calculate a Wasserstein-type distance between several 5-year-step GMM pairs, including [2011 v.s. 2016], [2012 v.s. 2017],  [2013 v.s. 2018], [2014 v.s. 2019], [2015 v.s. 2020]. Therefore, envolutionary changes over five years can be described.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "from sklearn import mixture\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "import scipy.io as sio\n",
    "from pyomo.environ import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from random import random\n",
    "from scipy.stats import multivariate_normal\n",
    "from scipy.stats import matrix_normal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sps\n",
    "import scipy.linalg as spl\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "import ot\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Paths to directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Core data directory (common files)\n",
    "data_dir = os.path.abspath(os.path.join(os.path.curdir, os.path.pardir, os.path.pardir, 'data'))\n",
    "\n",
    "# MMSDM data directory\n",
    "MMSDMdata_dir = os.path.join(data_dir,'AUS2009_2020')\n",
    "\n",
    "# Network directory\n",
    "network_dir = os.path.abspath(os.path.join(os.path.curdir, os.path.pardir, '1_network'))\n",
    "\n",
    "# Generators directory\n",
    "gens_dir = os.path.abspath(os.path.join(os.path.curdir, os.path.pardir, '2_generators'))\n",
    "\n",
    "# Output directory\n",
    "output_dir = os.path.abspath(os.path.join(os.path.curdir, 'output'))\n",
    "output_for_mat_dir=os.path.abspath(os.path.join(os.path.curdir, 'output_for_mat'))\n",
    "\n",
    "# basic DataFrame for network (index=NODE_ID)\n",
    "df_n = pd.read_csv(os.path.join(network_dir, 'output', 'network_nodes.csv'), index_col='NODE_ID', dtype={'NEAREST_NODE':np.int32})\n",
    "\n",
    "# basic DataFrame for generator (index=DUID)\n",
    "df_g = pd.read_csv(os.path.join(gens_dir, 'output', 'generators.csv'), dtype={'NODE': int}, index_col='DUID')\n",
    "\n",
    "# population information \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def Standardization(df_population):\n",
    "    L=np.linalg.cholesky(df_population.cov().values)\n",
    "    invL=np.linalg.inv(L)\n",
    "    \n",
    "    Zsample=np.dot(invL,(df_population-df_population.mean()).values.T)\n",
    "    df_z=pd.DataFrame(Zsample).T\n",
    "\n",
    "    return df_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit GMM \n",
    "\n",
    "These GMM paramters would be used for measuring the \"distance\" between current probability distribution and future probability distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def GMM_fit(df_population):\n",
    "    # Fit the GMM and save the GMM parameters as distionary and mat. file\n",
    "    clf = mixture.GaussianMixture(n_components=3, covariance_type='full')\n",
    "    clf.fit(df_population)\n",
    "\n",
    "    GMM={}\n",
    "    GMM['Pr']=clf.weights_# Pr for each components\n",
    "    GMM['means']=clf.means_\n",
    "    GMM['Covariances']=clf.covariances_\n",
    "    return GMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following codes are downloaded from [5]: Wasserstein-type distance betwwen two GMMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wasserstein distance betwwen two Guassians\n",
    "def GaussianW2(m0,m1,Sigma0,Sigma1):\n",
    "    # compute the quadratic Wasserstein distance between two Gaussians with means m0 and m1 and covariances Sigma0 and Sigma1\n",
    "    Sigma00  = spl.sqrtm(Sigma0)\n",
    "    Sigma010 = spl.sqrtm(Sigma00@Sigma1@Sigma00)\n",
    "    d        = np.linalg.norm(m0-m1)**2+np.trace(Sigma0+Sigma1-2*Sigma010)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Wasserstein-type distance betwwen two GMMs\n",
    "def GW2(pi0,pi1,mu0,mu1,S0,S1):\n",
    "    # return the GW2 discrete map and the GW2 distance between two GMMs\n",
    "    K0 = mu0.shape[0]\n",
    "    K1 = mu1.shape[0]\n",
    "    d  = mu0.shape[1]\n",
    "    S0 = S0.reshape(K0,d,d)\n",
    "    S1 = S1.reshape(K1,d,d)\n",
    "    M  = np.zeros((K0,K1))\n",
    "    # First we compute the distance matrix between all Gaussians pairwise\n",
    "    for k in range(K0):\n",
    "        for l in range(K1):\n",
    "            M[k,l]  = GaussianW2(mu0[k,:],mu1[l,:],S0[k,:,:],S1[l,:,:])\n",
    "    # Then we compute the OT distance or OT map thanks to the OT library\n",
    "    wstar     = ot.emd(pi0,pi1,M)         # discrete transport plan\n",
    "    distGW2   = np.sum(wstar*M)\n",
    "    return distGW2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duration of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "month_list=['0101','0201','0301','0401','0501','0601','0701','0801','0901','1001','1101','1201']\n",
    "\n",
    "#year_list1_load=['2009','2010','2011','2012','2013','2014']\n",
    "#year_list1_wind=['2010','2011','2012','2013','2014']\n",
    "year_list1=['2011','2012','2013','2014','2015']\n",
    "year_list2=['2016','2017','2018','2019','2020']\n",
    "\n",
    "Missing_list=['20090101','20090201','20090301','20090401','20090501','20090601','20150601','20160101','20170701','20170801','20170901','20200601','20200701','20201201']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get dispatch data of wind power\n",
    "1. Every dispatched units have an ID (i.e., DUID) while we attached the fuel type from 'df_g', so that wind power dispatch data can be extracted;\n",
    "2. Parse and save unit dispatch data. Note that dispatch in MW is given at 5min intervals, and that the time resolution of demand data is 30min intervals, corresponding to the length of a trading period in the NEM. To align the time resolution of these signals unit dispatch data are aggregated, with mean power output over 60min intervals computed for each DUID.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the four wind power data\n",
    "def get_zonal_wind_power (df_DISPATCH_UNIT_SCADA):\n",
    "    # Convert to datetime objects\n",
    "    df_DISPATCH_UNIT_SCADA['SETTLEMENTDATE'] = pd.to_datetime(df_DISPATCH_UNIT_SCADA['SETTLEMENTDATE'])\n",
    "    # Pivot dataframe. Dates are the index values, columns are DUIDs, values are DUID dispatch levels\n",
    "    df_DISPATCH_UNIT_SCADA_piv = df_DISPATCH_UNIT_SCADA.pivot(index='SETTLEMENTDATE', columns='DUID', values='SCADAVALUE')\n",
    "    # To ensure the 30th minute interval is included during each trading interval the time index is offset\n",
    "    # by 1min. Once the groupby operation is performed this offset is removed.\n",
    "    df_DISPATCH_UNIT_SCADA_agg = df_DISPATCH_UNIT_SCADA_piv.groupby(pd.Grouper(freq='60Min', base=1, label='right')).mean()\n",
    "    df_DISPATCH_UNIT_SCADA_agg = df_DISPATCH_UNIT_SCADA_agg.set_index(df_DISPATCH_UNIT_SCADA_agg.index - pd.Timedelta(minutes=1))\n",
    "    df_DISPATCH_UNIT_SCADA_agg\n",
    "    # (1)Nodal renewable energy system (RES) disaptch;(2)NEM Zonal RES dispatch\n",
    "    # Add fuel category to each DUID in SCADA dispatch dataframe\n",
    "    df_DISPATCH_UNIT_SCADA_agg = df_DISPATCH_UNIT_SCADA_agg.T.join(df_g[['FUEL_CAT']])\n",
    "    # Only consider intermittent solar and wind generators\n",
    "    mask = df_DISPATCH_UNIT_SCADA_agg['FUEL_CAT'].isin(['Wind'])\n",
    "    # Keep wind and solar (RES) DUIDs, drop fuel category column, and transpose (columns=DUID, index=Timestamp)\n",
    "    # All intermittent generation profiles\n",
    "    # (columns=DUID, index=Timestamps)\n",
    "    df_DUID_RES = df_DISPATCH_UNIT_SCADA_agg[mask].drop('FUEL_CAT', axis=1).T  \n",
    "\n",
    "    #(1) Nodal RES\n",
    "    # Add node to which generator is connected, groupby node and sum, \n",
    "    # reindex columns using all node IDs, yielding total intermittent injection at each node\n",
    "    # Injections from intermittent sources (columns=node ID, index=Timestamps)\n",
    "    df_nodal_RES=df_DUID_RES.T.join(df_g[['NODE']], how='left').groupby('NODE').sum().T.reindex(columns=df_n.index, fill_value=0)\n",
    "\n",
    "    df_regional_RES=df_nodal_RES.T.join(df_n[['NEM_REGION']], how='left').groupby('NEM_REGION').sum().T.reindex(columns=df_n['NEM_REGION'].unique(), fill_value=0)\n",
    "\n",
    "    #(2)Zonal RES\n",
    "    #Fixed injections from intermittent sources for each NEM zone (columns=Zone ID, index=Timestamps\n",
    "    df_zonal_RES=df_nodal_RES.T.join(df_n[['NEM_ZONE']], how='left').groupby('NEM_ZONE').sum().T.reindex(columns=df_n['NEM_ZONE'].unique(), fill_value=0)\n",
    "    \n",
    "    #(3) return the selected RESs for IEEE 24 model\n",
    "    df_wind=df_zonal_RES[['NSA']]\n",
    "    return df_wind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get variable injections in one year\n",
    " Load data in each NEM region are given at 30min intervals. Likewise, demand data are aggregated, with mean values over 60min intervals computed for each region. <br />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_STDdata_oneYear(year):\n",
    "        df_Pv=pd.DataFrame()\n",
    "        df_load_5Region=pd.DataFrame()\n",
    "        df_wind=pd.DataFrame(columns = ['NSA']) \n",
    "        \n",
    "        for month in month_list:\n",
    "            a=year+''+month\n",
    "            if a in Missing_list:\n",
    "                continue\n",
    "            print(a)    \n",
    "            #-------Load data-----------------\n",
    "            name_csv='PUBLIC_DVD_TRADINGREGIONSUM_201901010000.CSV'\n",
    "            name_csv=name_csv.replace('0101',month)\n",
    "            name_csv=name_csv.replace('2019',year)\n",
    "            # Regional summary for each trading interval\n",
    "            df_TRADINGREGIONSUM = pd.read_csv(os.path.join(MMSDMdata_dir, name_csv),\n",
    "                                              skiprows=1, skipfooter=1, engine='python')\n",
    "            # Convert settlement date to datetime\n",
    "            df_TRADINGREGIONSUM['SETTLEMENTDATE'] = pd.to_datetime(df_TRADINGREGIONSUM['SETTLEMENTDATE'])\n",
    "            # Pivot dataframe. Index is timestamp, columns are NEM region IDs, values are total demand\n",
    "            df_TRADINGREGIONSUM_piv = df_TRADINGREGIONSUM.pivot(index='SETTLEMENTDATE', columns='REGIONID', values='TOTALDEMAND')\n",
    "            df_TRADINGREGIONSUM_piv = df_TRADINGREGIONSUM_piv.groupby(pd.Grouper(freq='60Min', base=1, label='right')).mean()\n",
    "            df_TRADINGREGIONSUM_piv = df_TRADINGREGIONSUM_piv.set_index(df_TRADINGREGIONSUM_piv.index - pd.Timedelta(minutes=1))\n",
    "            \n",
    "            df_load_5Region=df_load_5Region.append(df_TRADINGREGIONSUM_piv)\n",
    "            \n",
    "            #-------Wind data-----------------\n",
    "            name_csv='PUBLIC_DVD_DISPATCH_UNIT_SCADA_201901010000.CSV'\n",
    "            name_csv=name_csv.replace('0101',month)\n",
    "            name_csv=name_csv.replace('2019',year)\n",
    "        \n",
    "            df_DISPATCH_UNIT_SCADA = pd.read_csv(os.path.join(MMSDMdata_dir, name_csv),\n",
    "                                         skiprows=1, skipfooter=1, engine='python')\n",
    "    \n",
    "            df_wind0=get_zonal_wind_power (df_DISPATCH_UNIT_SCADA)            \n",
    "            df_wind=df_wind.append(df_wind0)\n",
    "        \n",
    "\n",
    "        #-------Variable injections--------\n",
    "        df_Pv = pd.concat([df_load_5Region, df_wind], axis=1)\n",
    "            \n",
    "        # Standardization\n",
    "        df_z=Standardization(df_Pv)\n",
    "                    \n",
    "        return  df_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analyze_norms(df_z,year):\n",
    "    df_z_norm1=df_z.apply(lambda x: np.linalg.norm(x,ord=1),axis=1)\n",
    "    df_z_norm2=df_z.apply(lambda x: np.linalg.norm(x,ord=2),axis=1)\n",
    "            \n",
    "    df_norm1_DiffR=pd.DataFrame(columns=['1','2','3','4','5','6','7','8'])\n",
    "    for i in range(1,9):\n",
    "            mask=df_z_norm2<=i\n",
    "            z_norm1_sub=df_z_norm1[mask]\n",
    "            df_norm1_DiffR.loc['norm1_Year='+str(year),str(i)]=np.percentile(z_norm1_sub,95)                 \n",
    "            df_norm1_DiffR.loc['Pr_Year='+str(year),str(i)]=len(z_norm1_sub)/len(df_z_norm1)\n",
    "            \n",
    "    print(df_norm1_DiffR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_data_oneYear(year):\n",
    "        df_load_5Region=pd.DataFrame()\n",
    "        df_wind=pd.DataFrame(columns = ['NSA']) \n",
    "        \n",
    "        for month in month_list:\n",
    "            a=year+''+month\n",
    "            if a in Missing_list:\n",
    "                continue\n",
    "            print(a)    \n",
    "            #-------Load data-----------------\n",
    "            name_csv='PUBLIC_DVD_TRADINGREGIONSUM_201901010000.CSV'\n",
    "            name_csv=name_csv.replace('0101',month)\n",
    "            name_csv=name_csv.replace('2019',year)\n",
    "            # Regional summary for each trading interval\n",
    "            df_TRADINGREGIONSUM = pd.read_csv(os.path.join(MMSDMdata_dir, name_csv),\n",
    "                                              skiprows=1, skipfooter=1, engine='python')\n",
    "            # Convert settlement date to datetime\n",
    "            df_TRADINGREGIONSUM['SETTLEMENTDATE'] = pd.to_datetime(df_TRADINGREGIONSUM['SETTLEMENTDATE'])\n",
    "            # Pivot dataframe. Index is timestamp, columns are NEM region IDs, values are total demand\n",
    "            df_TRADINGREGIONSUM_piv = df_TRADINGREGIONSUM.pivot(index='SETTLEMENTDATE', columns='REGIONID', values='TOTALDEMAND')\n",
    "            df_TRADINGREGIONSUM_piv = df_TRADINGREGIONSUM_piv.groupby(pd.Grouper(freq='60Min', base=1, label='right')).mean()\n",
    "            df_TRADINGREGIONSUM_piv = df_TRADINGREGIONSUM_piv.set_index(df_TRADINGREGIONSUM_piv.index - pd.Timedelta(minutes=1))\n",
    "            \n",
    "            df_load_5Region=df_load_5Region.append(df_TRADINGREGIONSUM_piv)\n",
    "            \n",
    "            #-------Wind data-----------------\n",
    "            name_csv='PUBLIC_DVD_DISPATCH_UNIT_SCADA_201901010000.CSV'\n",
    "            name_csv=name_csv.replace('0101',month)\n",
    "            name_csv=name_csv.replace('2019',year)\n",
    "        \n",
    "            df_DISPATCH_UNIT_SCADA = pd.read_csv(os.path.join(MMSDMdata_dir, name_csv),\n",
    "                                         skiprows=1, skipfooter=1, engine='python')\n",
    "    \n",
    "            df_wind0=get_zonal_wind_power (df_DISPATCH_UNIT_SCADA)            \n",
    "            df_wind=df_wind.append(df_wind0)\n",
    "        \n",
    "\n",
    "            #-------Variable injections--------\n",
    "            df_Pv = pd.concat([df_load_5Region, df_wind], axis=1)\n",
    "            \n",
    "            # Standardization and analyze\n",
    "            df_z=Standardization(df_Pv)\n",
    "                    \n",
    "            # Fit GMM\n",
    "            GMM_z=GMM_fit(df_z)\n",
    "            \n",
    "        return GMM_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20110101\n",
      "20110201\n",
      "20110301\n",
      "20110401\n",
      "20110501\n",
      "20110601\n",
      "20110701\n",
      "20110801\n",
      "20110901\n",
      "20111001\n",
      "20111101\n",
      "20111201\n",
      "                        1         2         3         4         5         6  \\\n",
      "norm1_Year=2011   2.13493   4.23609   5.97689   6.85137   7.25929   7.41814   \n",
      "Pr_Year=2011     0.010274  0.359589  0.867123  0.964612  0.986644  0.994292   \n",
      "\n",
      "                        7         8  \n",
      "norm1_Year=2011   7.51801    7.5747  \n",
      "Pr_Year=2011     0.997831  0.999429  \n",
      "20160201\n",
      "20160301\n",
      "20160401\n",
      "20160501\n",
      "20160601\n",
      "20160701\n",
      "20160801\n",
      "20160901\n",
      "20161001\n",
      "20161101\n",
      "20161201\n",
      "                         1         2        3         4         5         6  \\\n",
      "norm1_Year=2016    2.15646   4.23131  5.92391   6.89868   7.37691   7.61424   \n",
      "Pr_Year=2016     0.0110867  0.351365  0.86172  0.965765  0.989035  0.996954   \n",
      "\n",
      "                        7         8  \n",
      "norm1_Year=2016    7.6649   7.67938  \n",
      "Pr_Year=2016     0.999025  0.999756  \n",
      "1.0598880712990255\n",
      "20120101\n",
      "20120201\n",
      "20120301\n",
      "20120401\n",
      "20120501\n",
      "20120601\n",
      "20120701\n",
      "20120801\n",
      "20120901\n",
      "20121001\n",
      "20121101\n",
      "20121201\n",
      "                          1         2         3         4         5         6  \\\n",
      "norm1_Year=2012     2.16452   4.27401   5.95528   6.96883   7.30272   7.46195   \n",
      "Pr_Year=2012     0.00865209  0.339139  0.858379  0.967555  0.989071  0.997268   \n",
      "\n",
      "                        7        8  \n",
      "norm1_Year=2012   7.53263  7.54206  \n",
      "Pr_Year=2012     0.999658        1  \n",
      "20170101\n",
      "20170201\n",
      "20170301\n",
      "20170401\n",
      "20170501\n",
      "20170601\n",
      "20171001\n",
      "20171101\n",
      "20171201\n",
      "                         1        2         3        4         5         6  \\\n",
      "norm1_Year=2017    2.16978  4.20411   5.81825  6.81153   7.44793   7.96225   \n",
      "Pr_Year=2017     0.0216728  0.43605  0.844322  0.94826  0.979396  0.996032   \n",
      "\n",
      "                        7        8  \n",
      "norm1_Year=2017   8.06331  8.09321  \n",
      "Pr_Year=2017     0.999389        1  \n",
      "1.4257790678840028\n",
      "20130101\n",
      "20130201\n",
      "20130301\n",
      "20130401\n",
      "20130501\n",
      "20130601\n",
      "20130701\n",
      "20130801\n",
      "20130901\n",
      "20131001\n",
      "20131101\n",
      "20131201\n",
      "                         1         2         3         4         5         6  \\\n",
      "norm1_Year=2013    2.20009    4.2221   5.87904   6.89017   7.42459   7.69324   \n",
      "Pr_Year=2013     0.0130137  0.388927  0.859361  0.957534  0.985502  0.995776   \n",
      "\n",
      "                        7         8  \n",
      "norm1_Year=2013   7.77753   7.80704  \n",
      "Pr_Year=2013     0.998744  0.999658  \n",
      "20180101\n",
      "20180201\n",
      "20180301\n",
      "20180401\n",
      "20180501\n",
      "20180601\n",
      "20180701\n",
      "20180801\n",
      "20180901\n",
      "20181001\n",
      "20181101\n",
      "20181201\n",
      "                         1         2         3        4         5        6  \\\n",
      "norm1_Year=2018    2.14665   4.25402   5.91653  7.04117   7.52499  7.70485   \n",
      "Pr_Year=2018     0.0178082  0.399543  0.842466  0.96153  0.986301  0.99395   \n",
      "\n",
      "                        7        8  \n",
      "norm1_Year=2018   7.80795  7.86345  \n",
      "Pr_Year=2018     0.998288        1  \n",
      "1.3114923383494521\n",
      "20140101\n",
      "20140201\n",
      "20140301\n",
      "20140401\n",
      "20140501\n",
      "20140601\n",
      "20140701\n",
      "20140801\n",
      "20140901\n",
      "20141001\n",
      "20141101\n",
      "20141201\n",
      "                         1         2         3         4         5         6  \\\n",
      "norm1_Year=2014    2.13846   4.24052   5.92489    6.9123   7.27273   7.51792   \n",
      "Pr_Year=2014     0.0166667  0.402169  0.849772  0.959361  0.982648  0.993721   \n",
      "\n",
      "                        7        8  \n",
      "norm1_Year=2014   7.67238  7.69118  \n",
      "Pr_Year=2014     0.999315        1  \n",
      "20190101\n",
      "20190201\n",
      "20190301\n",
      "20190401\n",
      "20190501\n",
      "20190601\n",
      "20190701\n",
      "20190801\n",
      "20190901\n",
      "20191001\n",
      "20191101\n",
      "20191201\n",
      "                         1         2         3         4         5         6  \\\n",
      "norm1_Year=2019    2.17806   4.22332   5.85901   6.91444   7.46655   7.76927   \n",
      "Pr_Year=2019     0.0157534  0.429795  0.847489  0.954795  0.980822  0.993493   \n",
      "\n",
      "                        7         8  \n",
      "norm1_Year=2019   7.91167    7.9657  \n",
      "Pr_Year=2019     0.998744  0.999886  \n",
      "0.6258466112967942\n",
      "20150101\n",
      "20150201\n",
      "20150301\n",
      "20150401\n",
      "20150501\n",
      "20150701\n",
      "20150801\n",
      "20150901\n",
      "20151001\n",
      "20151101\n",
      "20151201\n",
      "                         1         2         3         4         5         6  \\\n",
      "norm1_Year=2015    2.15702   4.23029   5.91633   7.12227   7.61851   7.84985   \n",
      "Pr_Year=2015     0.0119403  0.389303  0.838433  0.959701  0.987687  0.999005   \n",
      "\n",
      "                        7        8  \n",
      "norm1_Year=2015    7.8776  7.88599  \n",
      "Pr_Year=2015     0.999876        1  \n",
      "20200101\n",
      "20200201\n",
      "20200301\n",
      "20200401\n",
      "20200501\n",
      "20200801\n",
      "20200901\n",
      "20201001\n",
      "20201101\n",
      "                         1         2         3         4        5         6  \\\n",
      "norm1_Year=2020    2.13167   4.21341   5.92911   7.05281  7.46286   7.66067   \n",
      "Pr_Year=2020     0.0206813  0.424574  0.843674  0.958029  0.98312  0.991788   \n",
      "\n",
      "                        7         8  \n",
      "norm1_Year=2020   7.78328   7.86068  \n",
      "Pr_Year=2020     0.997415  0.999848  \n",
      "1.4475914231150309\n"
     ]
    }
   ],
   "source": [
    "distGMM_set = np.zeros((5,3))\n",
    "for i in range(0,5):\n",
    "        y1=year_list1[i]\n",
    "        y2=year_list2[i]    \n",
    "\n",
    "        \n",
    "        df_z1=get_STDdata_oneYear(y1)\n",
    "        GMM_z1=GMM_fit(df_z1)\n",
    "        Analyze_norms(df_z1,y1)\n",
    "        \n",
    "        df_z2=get_STDdata_oneYear(y2)\n",
    "        GMM_z2=GMM_fit(df_z2)\n",
    "        Analyze_norms(df_z2,y2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        pi0=GMM_z1['Pr']\n",
    "        pi1=GMM_z2['Pr']\n",
    "        mu0=GMM_z1['means']\n",
    "        mu1=GMM_z2['means']\n",
    "        \n",
    "        if y1=='2012':\n",
    "            S0=GMM_z1['Covariances']-1e-6\n",
    "            S1=GMM_z2['Covariances']-1e-6\n",
    "        else:\n",
    "            S0=GMM_z1['Covariances']\n",
    "            S1=GMM_z2['Covariances']        \n",
    "\n",
    "        distGW2=GW2(pi0,pi1,mu0,mu1,S0,S1)\n",
    "        distGMM_set[i,0]=y1\n",
    "        distGMM_set[i,1]=y2\n",
    "        distGMM_set[i,2]=sqrt(distGW2)\n",
    "        \n",
    "        print(sqrt(distGW2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of final simulation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year1</th>\n",
       "      <th>Year2</th>\n",
       "      <th>Wasserstein-type distance betwwen two GMMs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.059888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>2017.0</td>\n",
       "      <td>1.425779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>1.311492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.625847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>1.447591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year1   Year2  Wasserstein-type distance betwwen two GMMs\n",
       "0  2011.0  2016.0                                    1.059888\n",
       "1  2012.0  2017.0                                    1.425779\n",
       "2  2013.0  2018.0                                    1.311492\n",
       "3  2014.0  2019.0                                    0.625847\n",
       "4  2015.0  2020.0                                    1.447591"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_distGMM_set=pd.DataFrame(distGMM_set)\n",
    "df_distGMM_set.columns=['Year1','Year2','Wasserstein-type distance betwwen two GMMs']\n",
    "df_distGMM_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean value of Wasserstein-type distance betwwen two GMMs, which is used in our simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1741195023888609"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(distGMM_set[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] -Xenophon, A., Hill, D. Open grid model of Australia’s National Electricity Market allowing backtesting against historic data. Sci Data 5, 180203 (2018). https://doi.org/10.1038/sdata.2018.203\n",
    "\n",
    "[2] -Xenophon, A. K. Geospatial Modelling of Australia’s National Electricity Market. GitHub https://github.com/akxen/egrimod-nem (2018).\n",
    "\n",
    "[3] -Australian Energy Markets Operator. Data Archive (2018). at http://www.nemweb.com.au/#mms-data-model \n",
    "\n",
    "[4] -Australian Energy Markets Operator. Data Archive (2018). at https://nemweb.com.au/Data_Archive/Wholesale_Electricity/MMSDM/\n",
    "\n",
    "\n",
    "[5] https://github.com/judelo/gmmot\n",
    "#  Datasets List from MMSDM\n",
    "A summary of the tables used from AEMO's MMSDM database [3] is given below:\n",
    "\n",
    "| Table | Description |\n",
    "| :----- | :----- |\n",
    "|DISPATCH_UNIT_SCADA | MW dispatch at 5 minute (dispatch) intervals for DUIDs within the NEM.|\n",
    "|TRADINGREGIONSUM | Contains load in each NEM region at 30 minute (trading) intervals.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (egrimod-nem-env)",
   "language": "python",
   "name": "egrimod-nem-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
